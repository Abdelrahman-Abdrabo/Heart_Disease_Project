# -*- coding: utf-8 -*-
"""train_export_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OaFiHu9T972MKLjGmZ33m-dYvBcTBQd0
"""

!pip install ucimlrepo -qq
import pandas as pd
import numpy as np
import joblib
from ucimlrepo import fetch_ucirepo
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC

# -----------------------------
# 1. Load Raw Data from UCI
# -----------------------------
heart_disease = fetch_ucirepo(id=45)
X = heart_disease.data.features
y = heart_disease.data.targets["num"]

# Binarize target (0 = no disease, 1 = disease)
y = (y > 0).astype(int)

# -----------------------------
# 2. Define preprocessing
# -----------------------------
numeric_features = ["age", "trestbps", "chol", "thalach", "oldpeak", "ca"]
categorical_features = ["sex", "cp", "fbs", "restecg", "exang", "slope", "thal"]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ],
    remainder='passthrough',
    verbose_feature_names_out=False
)

# -----------------------------
# 3. Final selected features (from Phase 2.3)
# -----------------------------
selected_features = [
  'exang_0',
  'exang_1','slope_2',
  'cp_4', 'cp_1',
  'ca', 'slope_1',
  'thal_3.0', 'oldpeak',
  'age', 'sex_0',
  'cp_3', 'thalach',
  'trestbps', 'chol',
  'thal_7.0']

# Transformer to select only these features by name
feature_selector = ColumnTransformer(
    transformers=[('selector', 'passthrough', selected_features)],
    remainder='drop' # Drop any columns not in selected_features
)


# -----------------------------
# 4. Full Pipeline
# -----------------------------
pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("feature_selector", feature_selector),
    ("svm", SVC(probability=True, random_state=42))
]).set_output(transform="pandas") # Ensure the pipeline outputs a pandas DataFrame

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


param_dist = {
    "svm__C": np.logspace(-2, 2, 20),
    "svm__gamma": np.logspace(-3, 1, 20),
    "svm__kernel": ["rbf", "poly", "sigmoid"]
}

random_search = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_dist,
    n_iter=100,
    cv=5,
    # scoring="roc_auc",
    n_jobs=-1,
    verbose=2,
    random_state=42
)


random_search.fit(X_train, y_train)

print("Best Parameters:", random_search.best_params_)
print("Best CV Score:", random_search.best_score_)

# -----------------------------
# 7. Save Final Model
# -----------------------------
joblib.dump(random_search.best_estimator_, "final_pipeline.pkl")
print("ðŸš€ Final pipeline (with preprocessing + feature selection + tuned SVM) saved as final_pipeline.pkl")

"""# Thanks"""